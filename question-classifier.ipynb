{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of Questions\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook is dedicated to a challenging task in the realm of Deep Learning - the classification of questions into two categories: sincere and insincere. Our dataset comprises over 250,000 training questions, each labeled as either honest or dishonest, providing a rich ground for training and testing our models.\n",
    "\n",
    "### Objective\n",
    "\n",
    "The primary goal is to implement and train models specifically on this dataset, evaluating their performance in accurately categorizing the questions. This task not only tests the practical application of deep learning models but also emphasizes the importance of precise data preprocessing and model tuning.\n",
    "\n",
    "### Approach\n",
    "\n",
    "We will explore the dataset, implement various models, and train them to classify questions effectively. The use of pre-trained embeddings is allowed, although the models themselves should be built and trained from scratch. Special attention will be paid to the preprocessing of text data, ensuring that every decision is data-driven and well-justified.\n",
    "\n",
    "### Evaluation Metrics\n",
    "\n",
    "The model's performance will be evaluated using metrics such as accuracy, precision, recall, and F1 score, with a particular focus on the F1 score due to the dataset's imbalance.\n",
    "\n",
    "### Tools and Technologies\n",
    "\n",
    "- Programming Language: Python\n",
    "- Main Libraries: PyTorch, Pandas, NumPy, Matplotlib, scikit-learn\n",
    "\n",
    "This notebook aims to not only build a robust model for question classification but also to serve as a comprehensive guide through the process of model selection, training, and evaluation in a real-world deep learning application.\n",
    "\n",
    "---\n",
    "\n",
    "**Note**: This project is an academic exercise, part of the \"Taller de Deep Learning\" course. It is intended for educational purposes, focusing on applying deep learning techniques to a real-world problem. The dataset and the task are designed to provide hands-on experience in building and evaluating deep learning models, particularly in the field of Natural Language Processing (NLP).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "### Setting Up the Conda Environment\n",
    "\n",
    "To ensure consistency across different platforms and manage dependencies effectively, we will use a Conda environment. If you haven't already installed Conda, please follow the instructions from [Miniconda](https://docs.conda.io/en/latest/miniconda.html) or [Anaconda](https://www.anaconda.com/products/distribution).\n",
    "\n",
    "#### Steps to Setup:\n",
    "\n",
    "1. **Create the Conda Environment**: Navigate to the root of the project where the `environment.yml` file is located and run the following command in your terminal:   \n",
    "```bash\n",
    "conda env create -f environment.yml\n",
    "```\n",
    "> This command will create a new Conda environment with all the necessary packages as specified in `environment.yml`.\n",
    "\n",
    "2. **Activate the Environment**: Once the environment is created, activate it using:\n",
    "```bash\n",
    "conda activate question-classifier-nlp\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "from itertools import chain\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the Random Seed for Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 43\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "\n",
    "# set deterministic cudnn for reproducibility\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
